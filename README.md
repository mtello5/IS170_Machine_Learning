# Decision Trees & Random Forests
Decision trees and random forests are common classification and regression techniques. Decision trees classify data by continually separating it into subgroups depending on feature values until a final judgment or class is obtained. The branches indicate feature values for each node in the tree. Using information gain or Gini impurity, the feature that best separates the data is used to build the tree. However, random forests use numerous decision trees to improve model accuracy and robustness. Random forests create decision trees using random subsets of features and observations. Combining all forest trees' forecasts yields the final prediction.

Application Examples:
* Diagnosing medical problems
* Used in customer profiling marketing
* Predicting loan defaults & frauds
